{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from skimage.transform import resize\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "from tensorflow.keras import layers\n",
        "import glob"
      ],
      "metadata": {
        "trusted": true,
        "id": "N6G5YnuAaM58"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-image"
      ],
      "metadata": {
        "trusted": true,
        "id": "8Lxws7mRaM59"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initialization"
      ],
      "metadata": {
        "id": "HFfDea56aM5-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "AUTO = tf.data.experimental.AUTOTUNE\n",
        "\n",
        "# Detect TPU, return appropriate distribution strategy\n",
        "try:\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "    print('Running on TPU ', tpu.master())\n",
        "except ValueError:\n",
        "    tpu = None\n",
        "\n",
        "if tpu:\n",
        "    tf.config.experimental_connect_to_cluster(tpu)\n",
        "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
        "else:\n",
        "    strategy = tf.distribute.get_strategy()\n",
        "\n",
        "print(\"REPLICAS: \", strategy.num_replicas_in_sync)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-08-21T19:16:13.835811Z",
          "iopub.execute_input": "2023-08-21T19:16:13.836949Z",
          "iopub.status.idle": "2023-08-21T19:16:21.749769Z",
          "shell.execute_reply.started": "2023-08-21T19:16:13.836907Z",
          "shell.execute_reply": "2023-08-21T19:16:21.748753Z"
        },
        "trusted": true,
        "id": "PSItSlRYaM5-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install cached-property\n",
        "from cached_property import cached_property\n",
        "from shutil import copyfile"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-08-21T19:16:21.762674Z",
          "iopub.execute_input": "2023-08-21T19:16:21.762938Z",
          "iopub.status.idle": "2023-08-21T19:16:26.725771Z",
          "shell.execute_reply.started": "2023-08-21T19:16:21.762913Z",
          "shell.execute_reply": "2023-08-21T19:16:26.724510Z"
        },
        "trusted": true,
        "id": "g8eFX1yOaM5_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from CTC_TPU import classic_ctc_loss"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-08-21T19:16:26.734208Z",
          "iopub.execute_input": "2023-08-21T19:16:26.734829Z",
          "iopub.status.idle": "2023-08-21T19:16:26.761994Z",
          "shell.execute_reply.started": "2023-08-21T19:16:26.734802Z",
          "shell.execute_reply": "2023-08-21T19:16:26.761193Z"
        },
        "trusted": true,
        "id": "PV7Xpn7waM5_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inpdir = \"/kaggle/input/asl-fingerspelling\"\n",
        "df = pd.read_csv(f'{inpdir}/train.csv')\n",
        "df[\"phrase_bytes\"] = df[\"phrase\"].map(lambda x: x.encode(\"utf-8\"))\n",
        "display(df.head())"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-08-21T19:16:26.768366Z",
          "iopub.execute_input": "2023-08-21T19:16:26.768688Z",
          "iopub.status.idle": "2023-08-21T19:16:26.990613Z",
          "shell.execute_reply.started": "2023-08-21T19:16:26.768661Z",
          "shell.execute_reply": "2023-08-21T19:16:26.989491Z"
        },
        "trusted": true,
        "id": "t89PSDg5aM5_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LIP = [\n",
        "    61, 185, 40, 39, 37, 267, 269, 270, 409,\n",
        "    291, 146, 91, 181, 84, 17, 314, 405, 321, 375,\n",
        "    78, 191, 80, 81, 82, 13, 312, 311, 310, 415,\n",
        "    95, 88, 178, 87, 14, 317, 402, 318, 324, 308,\n",
        "]\n",
        "\n",
        "LPOSEID = [13, 15, 17, 19, 21]\n",
        "RPOSEID = [14, 16, 18, 20, 22]\n",
        "POSEID = LPOSEID + RPOSEID\n",
        "\n",
        "FACE = [f'x_face_{i}' for i in LIP] + [f'y_face_{i}' for i in LIP] + [f'z_face_{i}' for i in LIP]\n",
        "LHAND = [f'x_left_hand_{i}' for i in range(21)] + [f'y_left_hand_{i}' for i in range(21)] + [f'z_left_hand_{i}' for i in range(21)]\n",
        "RHAND = [f'x_right_hand_{i}' for i in range(21)] + [f'y_right_hand_{i}' for i in range(21)] + [f'z_right_hand_{i}' for i in range(21)]\n",
        "POSE = [f'x_pose_{i}' for i in POSEID] + [f'y_pose_{i}' for i in POSEID] + [f'z_pose_{i}' for i in POSEID]\n",
        "\n",
        "SEL_COLS = FACE + LHAND + RHAND + POSE\n",
        "FRAME_LEN = 300\n",
        "\n",
        "X_IDX = [i for i, col in enumerate(SEL_COLS)  if \"x_\" in col]\n",
        "Y_IDX = [i for i, col in enumerate(SEL_COLS)  if \"y_\" in col]\n",
        "Z_IDX = [i for i, col in enumerate(SEL_COLS)  if \"z_\" in col]\n",
        "\n",
        "RHAND_IDX = [i for i, col in enumerate(SEL_COLS)  if \"right\" in col]\n",
        "LHAND_IDX = [i for i, col in enumerate(SEL_COLS)  if  \"left\" in col]\n",
        "RPOSE_IDX = [i for i, col in enumerate(SEL_COLS)  if  \"pose\" in col and int(col[-2:]) in RPOSEID]\n",
        "LPOSE_IDX = [i for i, col in enumerate(SEL_COLS)  if  \"pose\" in col and int(col[-2:]) in LPOSEID]\n",
        "\n",
        "FACE_IDX = [i for i, col in enumerate(SEL_COLS)  if \"face\" in col]\n",
        "HANDIDS = [i for i, _ in  enumerate(SEL_COLS) if 'hand' in _]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-08-21T19:16:26.991941Z",
          "iopub.execute_input": "2023-08-21T19:16:26.992275Z",
          "iopub.status.idle": "2023-08-21T19:16:27.004295Z",
          "shell.execute_reply.started": "2023-08-21T19:16:26.992247Z",
          "shell.execute_reply": "2023-08-21T19:16:27.003372Z"
        },
        "trusted": true,
        "id": "v7ZQC_XyaM6A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def remove_nan_rows_at_indices(x, column_indices):\n",
        "    mask = tf.math.logical_not(tf.reduce_all(tf.math.is_nan(tf.gather(x, column_indices, axis=1)), axis=1))\n",
        "    return tf.boolean_mask(x, mask)\n",
        "\n",
        "def drop_nan_timesteps(tensor, handids=HANDIDS):\n",
        "    handids = tf.convert_to_tensor(handids)\n",
        "    tensor_slice = tf.gather(tensor, handids, axis=1)\n",
        "    nan_mask = tf.reduce_all(tf.math.is_nan(tensor_slice), axis=1)\n",
        "    non_nan_mask = tf.logical_not(nan_mask)\n",
        "    filtered_tensor = tf.boolean_mask(tensor, non_nan_mask, axis=0)\n",
        "    return filtered_tensor\n",
        "\n",
        "\n",
        "def resize_pad(x):\n",
        "    if tf.shape(x)[0] < FRAME_LEN:\n",
        "        x = tf.pad(x, ([[0, FRAME_LEN-tf.shape(x)[0]], [0, 0]]))\n",
        "    else:\n",
        "        x = x[..., tf.newaxis]\n",
        "        x = tf.image.resize(x, (FRAME_LEN, tf.shape(x)[1]))\n",
        "        x = tf.squeeze(x, axis=-1)\n",
        "    return x"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-08-21T19:16:27.060669Z",
          "iopub.execute_input": "2023-08-21T19:16:27.060967Z",
          "iopub.status.idle": "2023-08-21T19:16:27.072734Z",
          "shell.execute_reply.started": "2023-08-21T19:16:27.060941Z",
          "shell.execute_reply": "2023-08-21T19:16:27.071813Z"
        },
        "trusted": true,
        "id": "1hp6TAzYaM6A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_IDX = [i for i, col in enumerate(SEL_COLS)  if \"x_\" in col]\n",
        "Y_IDX = [i for i, col in enumerate(SEL_COLS)  if \"y_\" in col]\n",
        "Z_IDX = [i for i, col in enumerate(SEL_COLS)  if \"z_\" in col]\n",
        "\n",
        "RHAND_IDX = [i for i, col in enumerate(SEL_COLS)  if \"right\" in col]\n",
        "LHAND_IDX = [i for i, col in enumerate(SEL_COLS)  if  \"left\" in col]\n",
        "RPOSE_IDX = [i for i, col in enumerate(SEL_COLS)  if  \"pose\" in col and int(col[-2:]) in RPOSEID]\n",
        "LPOSE_IDX = [i for i, col in enumerate(SEL_COLS)  if  \"pose\" in col and int(col[-2:]) in LPOSEID]\n",
        "\n",
        "FACE_IDX = [i for i, col in enumerate(SEL_COLS)  if \"face\" in col]\n",
        "HANDIDS = [i for i, _ in  enumerate(SEL_COLS) if 'hand' in _]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-08-21T19:16:27.073910Z",
          "iopub.execute_input": "2023-08-21T19:16:27.074212Z",
          "iopub.status.idle": "2023-08-21T19:16:27.089040Z",
          "shell.execute_reply.started": "2023-08-21T19:16:27.074186Z",
          "shell.execute_reply": "2023-08-21T19:16:27.088161Z"
        },
        "trusted": true,
        "id": "M-P67TVEaM6A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "FLIP_IDX_DICT = {}\n",
        "for i in range(len(RHAND_IDX)):\n",
        "    FLIP_IDX_DICT[RHAND_IDX[i]] = LHAND_IDX[i]\n",
        "    FLIP_IDX_DICT[LHAND_IDX[i]] = RHAND_IDX[i]\n",
        "\n",
        "for i in range(len(RPOSE_IDX)):\n",
        "    FLIP_IDX_DICT[RPOSE_IDX[i]] = LPOSE_IDX[i]\n",
        "    FLIP_IDX_DICT[LPOSE_IDX[i]] = RPOSE_IDX[i]\n",
        "\n",
        "for i in range(len(SEL_COLS)):\n",
        "    if i not in FLIP_IDX_DICT:\n",
        "        FLIP_IDX_DICT[i] = i"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-08-21T19:16:27.090084Z",
          "iopub.execute_input": "2023-08-21T19:16:27.090369Z",
          "iopub.status.idle": "2023-08-21T19:16:27.100768Z",
          "shell.execute_reply.started": "2023-08-21T19:16:27.090342Z",
          "shell.execute_reply": "2023-08-21T19:16:27.099870Z"
        },
        "trusted": true,
        "id": "-pshnSzKaM6A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IDX_TO_FLIP = RHAND_IDX + LHAND_IDX + RPOSE_IDX + LPOSE_IDX"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-08-21T19:16:27.101876Z",
          "iopub.execute_input": "2023-08-21T19:16:27.102167Z",
          "iopub.status.idle": "2023-08-21T19:16:27.110685Z",
          "shell.execute_reply.started": "2023-08-21T19:16:27.102142Z",
          "shell.execute_reply": "2023-08-21T19:16:27.109793Z"
        },
        "trusted": true,
        "id": "_OjY7EtUaM6A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "alph = [1 if i not in IDX_TO_FLIP else -1 for i in range(len(SEL_COLS))]\n",
        "ALP_TENSOR = tf.cast(tf.convert_to_tensor(alph)[None], tf.float32)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-08-21T19:16:27.111756Z",
          "iopub.execute_input": "2023-08-21T19:16:27.112049Z",
          "iopub.status.idle": "2023-08-21T19:16:27.128903Z",
          "shell.execute_reply.started": "2023-08-21T19:16:27.112023Z",
          "shell.execute_reply": "2023-08-21T19:16:27.127919Z"
        },
        "trusted": true,
        "id": "LpV9ZnwAaM6A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_flip_idx = [FLIP_IDX_DICT[i] for i in range(len(SEL_COLS))]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-08-21T19:16:27.130089Z",
          "iopub.execute_input": "2023-08-21T19:16:27.130382Z",
          "iopub.status.idle": "2023-08-21T19:16:27.148372Z",
          "shell.execute_reply.started": "2023-08-21T19:16:27.130355Z",
          "shell.execute_reply": "2023-08-21T19:16:27.147323Z"
        },
        "trusted": true,
        "id": "6546JsklaM6B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def flip_augm(x):\n",
        "    x = tf.gather(x, new_flip_idx, axis=1) * ALP_TENSOR\n",
        "    return x"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-08-21T19:16:27.149845Z",
          "iopub.execute_input": "2023-08-21T19:16:27.150163Z",
          "iopub.status.idle": "2023-08-21T19:16:27.171427Z",
          "shell.execute_reply.started": "2023-08-21T19:16:27.150135Z",
          "shell.execute_reply": "2023-08-21T19:16:27.170431Z"
        },
        "trusted": true,
        "id": "jVhmD_B9aM6B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def augm(x, phrace):\n",
        "    if tf.random.uniform(()) > 0.9:\n",
        "        x = flip_augm(x)\n",
        "    return x, phrace"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-08-21T19:16:27.172640Z",
          "iopub.execute_input": "2023-08-21T19:16:27.173037Z",
          "iopub.status.idle": "2023-08-21T19:16:27.192543Z",
          "shell.execute_reply.started": "2023-08-21T19:16:27.173008Z",
          "shell.execute_reply": "2023-08-21T19:16:27.191585Z"
        },
        "trusted": true,
        "id": "I-IXV0AaaM6B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MEAN = tf.convert_to_tensor(np.load('mean.npy'))\n",
        "STD = tf.convert_to_tensor(np.load('std.npy'))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-08-21T19:16:27.218715Z",
          "iopub.execute_input": "2023-08-21T19:16:27.219023Z",
          "iopub.status.idle": "2023-08-21T19:16:27.243641Z",
          "shell.execute_reply.started": "2023-08-21T19:16:27.218995Z",
          "shell.execute_reply": "2023-08-21T19:16:27.242725Z"
        },
        "trusted": true,
        "id": "8q-nmewaaM6B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load dataset"
      ],
      "metadata": {
        "id": "qiMWwDMdaM6B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 16 * strategy.num_replicas_in_sync\n",
        "\n",
        "with open (\"/kaggle/input/asl-fingerspelling/character_to_prediction_index.json\", \"r\") as f:\n",
        "    char_to_num = json.load(f)\n",
        "\n",
        "pad_token = 'P'\n",
        "pad_token_idx = 59\n",
        "\n",
        "char_to_num[pad_token] = pad_token_idx\n",
        "\n",
        "num_to_char = {j:i for i,j in char_to_num.items()}\n",
        "\n",
        "inpdir = \"/kaggle/input/asl-fingerspelling\"\n",
        "df = pd.read_csv(f'{inpdir}/train.csv')\n",
        "df_s = pd.read_csv(f'{inpdir}/supplemental_metadata.csv')\n",
        "\n",
        "table = tf.lookup.StaticHashTable(\n",
        "    initializer=tf.lookup.KeyValueTensorInitializer(\n",
        "        keys=list(char_to_num.keys()),\n",
        "        values=list(char_to_num.values()),\n",
        "    ),\n",
        "    default_value=tf.constant(-1),\n",
        "    name=\"class_weight\"\n",
        ")\n",
        "\n",
        "\n",
        "def filter_fn(record_bytes):\n",
        "    schema = {COL: tf.io.VarLenFeature(dtype=tf.float32) for COL in SEL_COLS}\n",
        "    schema[\"phrase\"] = tf.io.FixedLenFeature([], dtype=tf.string)\n",
        "    features = tf.io.parse_single_example(record_bytes, schema)\n",
        "    landmarks = ([tf.sparse.to_dense(features[COL]) for COL in SEL_COLS])\n",
        "    landmarks = tf.transpose(landmarks)\n",
        "    return tf.shape(landmarks)[0] > 5\n",
        "\n",
        "def decode_fn(record_bytes):\n",
        "    schema = {COL: tf.io.VarLenFeature(dtype=tf.float32) for COL in SEL_COLS}\n",
        "    schema[\"phrase\"] = tf.io.FixedLenFeature([], dtype=tf.string)\n",
        "    features = tf.io.parse_single_example(record_bytes, schema)\n",
        "    phrase = features[\"phrase\"]\n",
        "    landmarks = ([tf.sparse.to_dense(features[COL]) for COL in SEL_COLS])\n",
        "    landmarks = tf.transpose(landmarks)\n",
        "\n",
        "    landmarks = (landmarks - MEAN[None]) / STD[None]\n",
        "    landmarks = tf.where(tf.math.is_nan(landmarks), tf.zeros_like(landmarks), landmarks)\n",
        "    landmarks = resize_pad(landmarks)\n",
        "    landmarks = tf.reshape(landmarks, (FRAME_LEN, landmarks.shape[1]))\n",
        "\n",
        "    phrase = tf.strings.bytes_split(phrase)\n",
        "    phrase = table.lookup(phrase)\n",
        "    phrase = tf.pad(phrase, paddings=[[0, 64 - tf.shape(phrase)[0]]], constant_values = pad_token_idx)\n",
        "\n",
        "    return landmarks, phrase\n",
        "\n",
        "inpdir = \"/kaggle/working\"\n",
        "tffiles = list(df.file_id.map(lambda x: f'/kaggle/input/sign-language-recg-dataset-training/tfds/{x}.tfrecord').unique())\n",
        "PAD = pad_token_idx"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-08-21T20:33:53.713362Z",
          "iopub.execute_input": "2023-08-21T20:33:53.713837Z",
          "iopub.status.idle": "2023-08-21T20:33:53.926127Z",
          "shell.execute_reply.started": "2023-08-21T20:33:53.713803Z",
          "shell.execute_reply": "2023-08-21T20:33:53.924696Z"
        },
        "trusted": true,
        "id": "c--LDrSFaM6B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(tffiles)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-08-21T20:33:54.021087Z",
          "iopub.execute_input": "2023-08-21T20:33:54.021477Z",
          "iopub.status.idle": "2023-08-21T20:33:54.028591Z",
          "shell.execute_reply.started": "2023-08-21T20:33:54.021442Z",
          "shell.execute_reply": "2023-08-21T20:33:54.027342Z"
        },
        "trusted": true,
        "id": "uz4Snzw_aM6B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(17)\n",
        "np.random.shuffle(train_fls)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-08-21T20:33:54.675875Z",
          "iopub.execute_input": "2023-08-21T20:33:54.676854Z",
          "iopub.status.idle": "2023-08-21T20:33:54.681216Z",
          "shell.execute_reply.started": "2023-08-21T20:33:54.676823Z",
          "shell.execute_reply": "2023-08-21T20:33:54.680022Z"
        },
        "trusted": true,
        "id": "VYxDXNC_aM6B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = tf.data.TFRecordDataset(train_fls, num_parallel_reads=tf.data.AUTOTUNE, compression_type='GZIP').filter(filter_fn).map(decode_fn, num_parallel_calls=tf.data.AUTOTUNE).map(augm, num_parallel_calls=tf.data.AUTOTUNE).shuffle(5000).batch(batch_size, drop_remainder=True).prefetch(buffer_size=tf.data.AUTOTUNE).cache()\n",
        "batch = next(iter(train_dataset))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-08-21T20:33:55.315662Z",
          "iopub.execute_input": "2023-08-21T20:33:55.315970Z",
          "iopub.status.idle": "2023-08-21T20:33:58.634645Z",
          "shell.execute_reply.started": "2023-08-21T20:33:55.315944Z",
          "shell.execute_reply": "2023-08-21T20:33:58.633149Z"
        },
        "trusted": true,
        "id": "G7y-swkwaM6B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch[0].shape"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-08-21T20:33:58.636603Z",
          "iopub.execute_input": "2023-08-21T20:33:58.636909Z",
          "iopub.status.idle": "2023-08-21T20:34:08.583830Z",
          "shell.execute_reply.started": "2023-08-21T20:33:58.636880Z",
          "shell.execute_reply": "2023-08-21T20:34:08.582311Z"
        },
        "trusted": true,
        "id": "y8Bn86ygaM6B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define model"
      ],
      "metadata": {
        "id": "e-i0J9xGaM6C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def positional_encoding(length, depth):\n",
        "    depth = depth/2\n",
        "\n",
        "    positions = np.arange(length)[:, np.newaxis]\n",
        "    depths = np.arange(depth)[np.newaxis, :]/depth\n",
        "\n",
        "    angle_rates = 1 / (10000**depths)\n",
        "    angle_rads = positions * angle_rates\n",
        "\n",
        "    pos_encoding = np.concatenate(\n",
        "      [np.sin(angle_rads), np.cos(angle_rads)],\n",
        "      axis=-1)\n",
        "\n",
        "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
        "\n",
        "class PositionalEmbedding(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.pos_encoding = positional_encoding(length=2048, depth=d_model)\n",
        "\n",
        "    def call(self, x):\n",
        "        length = tf.shape(x)[1]\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x = x + self.pos_encoding[tf.newaxis, :length, :]\n",
        "        return x"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-08-21T19:16:38.665307Z",
          "iopub.execute_input": "2023-08-21T19:16:38.665604Z",
          "iopub.status.idle": "2023-08-21T19:16:38.680162Z",
          "shell.execute_reply.started": "2023-08-21T19:16:38.665577Z",
          "shell.execute_reply": "2023-08-21T19:16:38.679216Z"
        },
        "trusted": true,
        "id": "Q34NGjyWaM6C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, feed_forward_dim, rate=0.1):\n",
        "        super().__init__()\n",
        "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.ffn = keras.Sequential(\n",
        "            [\n",
        "                layers.Dense(feed_forward_dim, activation=\"swish\"),\n",
        "                layers.Dense(embed_dim),\n",
        "            ]\n",
        "        )\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = layers.Dropout(rate)\n",
        "        self.dropout2 = layers.Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        attn_output = self.att(inputs, inputs)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-08-21T19:16:38.695775Z",
          "iopub.execute_input": "2023-08-21T19:16:38.696090Z",
          "iopub.status.idle": "2023-08-21T19:16:38.706149Z",
          "shell.execute_reply.started": "2023-08-21T19:16:38.696065Z",
          "shell.execute_reply": "2023-08-21T19:16:38.705382Z"
        },
        "trusted": true,
        "id": "kR0fERkvaM6C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ECA(tf.keras.layers.Layer):\n",
        "    def __init__(self, kernel_size=5, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.supports_masking = True\n",
        "        self.kernel_size = kernel_size\n",
        "        self.conv = tf.keras.layers.Conv1D(1, kernel_size=kernel_size, strides=1, padding=\"same\", use_bias=False)\n",
        "\n",
        "    def call(self, inputs, mask=None):\n",
        "        nn = tf.keras.layers.GlobalAveragePooling1D()(inputs, mask=mask)\n",
        "        nn = tf.expand_dims(nn, -1)\n",
        "        nn = self.conv(nn)\n",
        "        nn = tf.squeeze(nn, -1)\n",
        "        nn = tf.nn.sigmoid(nn)\n",
        "        nn = nn[:,None,:]\n",
        "        return inputs * nn\n",
        "\n",
        "\n",
        "class CausalDWConv1D(tf.keras.layers.Layer):\n",
        "    def __init__(self,\n",
        "        kernel_size=17,\n",
        "        dilation_rate=1,\n",
        "        use_bias=False,\n",
        "        depthwise_initializer='glorot_uniform',\n",
        "        **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.causal_pad = tf.keras.layers.ZeroPadding1D((dilation_rate*(kernel_size-1),0))\n",
        "        self.dw_conv = tf.keras.layers.DepthwiseConv1D(\n",
        "                            kernel_size,\n",
        "                            strides=1,\n",
        "                            dilation_rate=dilation_rate,\n",
        "                            padding='valid',\n",
        "                            use_bias=use_bias,\n",
        "                            depthwise_initializer=depthwise_initializer)\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.causal_pad(inputs)\n",
        "        x = self.dw_conv(x)\n",
        "        return x\n",
        "\n",
        "class Conv1DBlock(tf.keras.layers.Layer):\n",
        "    def __init__(self,\n",
        "                 channel_size,\n",
        "                 kernel_size,\n",
        "                 dilation_rate=1,\n",
        "                 drop_rate=0.0,\n",
        "                 expand_ratio=2,\n",
        "                 se_ratio=0.25,\n",
        "                 activation='swish',\n",
        "                 **kwargs):\n",
        "        super(Conv1DBlock, self).__init__(**kwargs)\n",
        "\n",
        "        self.channel_size = channel_size\n",
        "        self.kernel_size = kernel_size\n",
        "        self.dilation_rate = dilation_rate\n",
        "        self.drop_rate = drop_rate\n",
        "        self.expand_ratio = expand_ratio\n",
        "        self.se_ratio = se_ratio\n",
        "        self.activation = activation\n",
        "\n",
        "        self.dense_expand = tf.keras.layers.Dense(\n",
        "            self.expand_ratio * channel_size,\n",
        "            use_bias=True,\n",
        "            activation=self.activation\n",
        "        )\n",
        "\n",
        "        self.dwconv = CausalDWConv1D(\n",
        "            self.kernel_size,\n",
        "            dilation_rate=self.dilation_rate,\n",
        "            use_bias=False\n",
        "        )\n",
        "\n",
        "        self.batch_norm = tf.keras.layers.BatchNormalization(momentum=0.95)\n",
        "\n",
        "        self.eca = ECA()\n",
        "\n",
        "        self.dense_project = tf.keras.layers.Dense(\n",
        "            self.channel_size,\n",
        "            use_bias=True\n",
        "        )\n",
        "\n",
        "        if self.drop_rate > 0:\n",
        "            self.dropout = tf.keras.layers.Dropout(self.drop_rate, noise_shape=(None,1,1))\n",
        "\n",
        "    def call(self, inputs):\n",
        "        channels_in = tf.keras.backend.int_shape(inputs)[-1]\n",
        "        channels_expand = channels_in * self.expand_ratio\n",
        "\n",
        "        x = self.dense_expand(inputs)\n",
        "\n",
        "        x = self.dwconv(x)\n",
        "\n",
        "        x = self.batch_norm(x)\n",
        "\n",
        "        x = self.eca(x)\n",
        "\n",
        "        x = self.dense_project(x)\n",
        "\n",
        "        if self.drop_rate > 0:\n",
        "            x = self.dropout(x)\n",
        "\n",
        "        if channels_in == self.channel_size:\n",
        "            x = tf.keras.layers.add([x, inputs])\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-08-21T19:16:38.707209Z",
          "iopub.execute_input": "2023-08-21T19:16:38.707478Z",
          "iopub.status.idle": "2023-08-21T19:16:38.731716Z",
          "shell.execute_reply.started": "2023-08-21T19:16:38.707453Z",
          "shell.execute_reply": "2023-08-21T19:16:38.730794Z"
        },
        "trusted": true,
        "id": "_8Pxlb3QaM6C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "class CnnTransformer(tf.keras.Model):\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_hid=64,\n",
        "        num_head=2,\n",
        "        num_feed_forward=128,\n",
        "        num_blocks=3,\n",
        "        maxlen=110,\n",
        "        num_classes=10,\n",
        "        ksizes=[11, 5, 3],\n",
        "        dim=256,\n",
        "        cnn_dropout=0.4,\n",
        "        dropout=0.15,\n",
        "        inp_dimen=200,\n",
        "        std_noise=0.1,\n",
        "        end_drop=0.2,\n",
        "        norm=True\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.maxlen = maxlen\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        self.pos_emb = PositionalEmbedding(d_model=num_hid)\n",
        "\n",
        "        self.encoders = [TransformerEncoder(num_hid, num_head, num_feed_forward, rate=dropout) for _ in range(num_blocks)]\n",
        "        self.cv_1d_blcs = [[Conv1DBlock(dim, ksizes[j], drop_rate=cnn_dropout) for j in range(len(ksizes))] for _ in range(num_blocks)]\n",
        "\n",
        "        self.classifier = layers.Dense(num_classes)\n",
        "        self.dns0 = tf.keras.layers.Dense(num_hid)\n",
        "        self.cv_per_block = len(ksizes)\n",
        "        self.num_blocs = num_blocks\n",
        "        self.mask = tf.keras.layers.Masking(mask_value=0.0)\n",
        "        self.pool1 = tf.keras.layers.MaxPooling1D()\n",
        "        self.pool2 = tf.keras.layers.MaxPooling1D()\n",
        "        self.noise = tf.keras.layers.GaussianNoise(std_noise)\n",
        "        self.btch_norm = layers.BatchNormalization()\n",
        "        self.norm = norm\n",
        "        self.drop_end=tf.keras.layers.Dropout(end_drop)\n",
        "\n",
        "    def encode(self, source, training=False):\n",
        "        x = self.noise(source)\n",
        "        x = self.dns0(x)\n",
        "        x = self.pos_emb(x)\n",
        "        if self.norm:\n",
        "            x = self.btch_norm(x)\n",
        "        for j in range(self.num_blocs):\n",
        "            for i in range(self.cv_per_block):\n",
        "                x = self.cv_1d_blcs[j][i](x)\n",
        "            x = self.encoders[j](x)\n",
        "            if j == 1 or j == 3:\n",
        "                x = self.pool1(x)\n",
        "        return x\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        x = self.mask(inputs)\n",
        "        #x = self.noise(x)\n",
        "        x = self.encode(x, training)\n",
        "        x = self.drop_end(x)\n",
        "        return self.classifier(x)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-08-21T19:26:08.869356Z",
          "iopub.execute_input": "2023-08-21T19:26:08.870392Z",
          "iopub.status.idle": "2023-08-21T19:26:08.891306Z",
          "shell.execute_reply.started": "2023-08-21T19:26:08.870347Z",
          "shell.execute_reply": "2023-08-21T19:26:08.890005Z"
        },
        "trusted": true,
        "id": "xbvRrF1saM6D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CTCLoss(keras.losses.Loss):\n",
        "\n",
        "    def __init__(self, reduction=tf.keras.losses.Reduction.NONE, name='ctc_loss'):\n",
        "        super().__init__(reduction=reduction, name=name)\n",
        "\n",
        "    def call(self, y_true, y_pred):\n",
        "        siz = batch_size // strategy.num_replicas_in_sync\n",
        "        y_true = tf.ensure_shape(y_true, (siz, 64))\n",
        "        y_pred = tf.ensure_shape(y_pred, (siz, 75, 60))\n",
        "        label_length = tf.cast(tf.argmax(y_true==pad_token_idx, axis = -1), dtype=\"int32\")#tf.cast(tf.ones([y_true.shape[0]]) * y_true.shape[0], dtype=\"int64\")\n",
        "        logit_length = tf.cast(tf.ones([siz]) * 75, dtype=\"int32\")\n",
        "\n",
        "        m = classic_ctc_loss(\n",
        "             labels=y_true,\n",
        "             logits=y_pred,\n",
        "             label_length=label_length,\n",
        "             logit_length=logit_length,\n",
        "             blank_index=pad_token_idx,\n",
        "         )\n",
        "        #tf.print(m)\n",
        "        #print(m)\n",
        "        return tf.math.reduce_mean(m)\n",
        "        #return tf.math.reduce_mean(loss)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-08-21T19:26:09.872596Z",
          "iopub.execute_input": "2023-08-21T19:26:09.873027Z",
          "iopub.status.idle": "2023-08-21T19:26:09.883316Z",
          "shell.execute_reply.started": "2023-08-21T19:26:09.872994Z",
          "shell.execute_reply": "2023-08-21T19:26:09.882006Z"
        },
        "trusted": true,
        "id": "KguCqMqhaM6D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch[0].shape"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-08-21T19:16:53.508270Z",
          "iopub.execute_input": "2023-08-21T19:16:53.508659Z",
          "iopub.status.idle": "2023-08-21T19:16:53.515153Z",
          "shell.execute_reply.started": "2023-08-21T19:16:53.508629Z",
          "shell.execute_reply": "2023-08-21T19:16:53.514201Z"
        },
        "trusted": true,
        "id": "oBpd6MbyaM6D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Init model"
      ],
      "metadata": {
        "id": "mTro_PauaM6D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with strategy.scope():\n",
        "    model = CnnTransformer(\n",
        "        num_hid=200,\n",
        "        num_head=6,\n",
        "        num_feed_forward=400,\n",
        "        maxlen=160,\n",
        "        num_blocks=6,\n",
        "        num_classes=60,\n",
        "        dim=200,\n",
        "        dropout=0.1,\n",
        "        cnn_dropout=0.4,\n",
        "        end_drop=0.1,\n",
        "        inp_dimen=273,\n",
        "        std_noise=0.005,\n",
        "        norm=False\n",
        "    )\n",
        "    opt = tf.keras.optimizers.Adam(0.0001)\n",
        "\n",
        "    model.compile(optimizer=opt, loss=CTCLoss())\n",
        "    _ = model(batch[0])\n",
        "    model.load_weights('/kaggle/working/with_max_pool/model_v_1_o.h5')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-08-21T19:26:13.870889Z",
          "iopub.execute_input": "2023-08-21T19:26:13.871372Z",
          "iopub.status.idle": "2023-08-21T19:26:27.976049Z",
          "shell.execute_reply.started": "2023-08-21T19:26:13.871339Z",
          "shell.execute_reply": "2023-08-21T19:26:27.974535Z"
        },
        "trusted": true,
        "id": "fG67cBjzaM6E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-08-21T19:26:27.977940Z",
          "iopub.execute_input": "2023-08-21T19:26:27.978355Z",
          "iopub.status.idle": "2023-08-21T19:26:28.100879Z",
          "shell.execute_reply.started": "2023-08-21T19:26:27.978326Z",
          "shell.execute_reply": "2023-08-21T19:26:28.099462Z"
        },
        "trusted": true,
        "id": "gBKuVluCaM6E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "kmROsE2BaM6E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "historadd_lossy = model.fit(train_dataset,\n",
        "                            #validation_data=val_dataset,\n",
        "                            #callbacks=[],\n",
        "                            epochs=20)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-08-21T19:26:28.102241Z",
          "iopub.execute_input": "2023-08-21T19:26:28.102643Z",
          "iopub.status.idle": "2023-08-21T19:58:14.427527Z",
          "shell.execute_reply.started": "2023-08-21T19:26:28.102613Z",
          "shell.execute_reply": "2023-08-21T19:58:14.426151Z"
        },
        "trusted": true,
        "id": "9NAEuquZaM6E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_weights('/kaggle/working/with_max_pool/model_v_2_0.h5')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-08-21T20:27:58.704610Z",
          "iopub.execute_input": "2023-08-21T20:27:58.705120Z",
          "iopub.status.idle": "2023-08-21T20:27:59.553252Z",
          "shell.execute_reply.started": "2023-08-21T20:27:58.705084Z",
          "shell.execute_reply": "2023-08-21T20:27:59.551796Z"
        },
        "trusted": true,
        "id": "490LfP6BaM6E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save as TFLite"
      ],
      "metadata": {
        "id": "40XhCUwNaM6G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_consecutive(tensor):\n",
        "    tensor_1 = tf.roll(tensor, shift=-1, axis=1)\n",
        "    mask = tensor != tensor_1\n",
        "    return tf.reshape(tf.boolean_mask(tensor, mask), [1, -1])\n",
        "\n",
        "def remove_number(tensor, num):\n",
        "    mask = tensor != num\n",
        "    return tf.reshape(tf.boolean_mask(tensor, mask), [1, -1])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-08-21T20:39:39.525451Z",
          "iopub.execute_input": "2023-08-21T20:39:39.525868Z",
          "iopub.status.idle": "2023-08-21T20:39:39.533583Z",
          "shell.execute_reply.started": "2023-08-21T20:39:39.525835Z",
          "shell.execute_reply": "2023-08-21T20:39:39.532251Z"
        },
        "trusted": true,
        "id": "UINIa-BSaM6G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PreprocessLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self):\n",
        "        super(PreprocessLayer, self).__init__()\n",
        "\n",
        "    def __call__(self, x):\n",
        "        x = (x - MEAN[None]) / STD[None]\n",
        "        x = tf.where(tf.math.is_nan(x), tf.zeros_like(x), x)\n",
        "        x = resize_pad(x)\n",
        "        x = tf.reshape(x, (300, 273))\n",
        "        return x\n",
        "\n",
        "\n",
        "class TFLiteModel(tf.Module):\n",
        "    def __init__(self, model):\n",
        "        super(TFLiteModel, self).__init__()\n",
        "\n",
        "        # Load the feature generation and main models\n",
        "        self.preprocess_layer = PreprocessLayer()\n",
        "        self.model = model\n",
        "\n",
        "    def decode(self, batch):\n",
        "        source = batch\n",
        "        return tf.convert_to_tensor(res, dtype=tf.int32)\n",
        "\n",
        "    @tf.function(input_signature=[tf.TensorSpec(shape=[None, len(SEL_COLS)], dtype=tf.float32, name='inputs')])\n",
        "    def __call__(self, inputs, training=False):\n",
        "        # Preprocess Data\n",
        "        x = self.preprocess_layer(inputs)\n",
        "        x = x[None]\n",
        "        preds = self.model(x, training=False)\n",
        "        idxs = tf.argmax(preds, -1)\n",
        "        x = remove_consecutive(idxs)\n",
        "        x = tf.cast(remove_number(x, 59), tf.int32)[0]\n",
        "\n",
        "        x = tf.one_hot(x, 59)\n",
        "        return {'outputs': x}\n",
        "\n",
        "pre = PreprocessLayer()\n",
        "print(pre(tf.zeros((300, 273))).shape)\n",
        "tflitemodel_base = TFLiteModel(model)\n",
        "print(batch[0][0].shape)\n",
        "tflitemodel_base(tf.zeros((300, 273)))[\"outputs\"].shape"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-08-21T20:39:56.395754Z",
          "iopub.execute_input": "2023-08-21T20:39:56.396157Z",
          "iopub.status.idle": "2023-08-21T20:40:00.451505Z",
          "shell.execute_reply.started": "2023-08-21T20:39:56.396125Z",
          "shell.execute_reply": "2023-08-21T20:40:00.450121Z"
        },
        "trusted": true,
        "id": "hwrmAkVgaM6G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prd = tf.math.argmax(model(batch[0][0][None]), axis=-1)\n",
        "tf.boolean_mask(prd, prd != 59)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-08-19T09:11:58.497140Z",
          "iopub.execute_input": "2023-08-19T09:11:58.497481Z",
          "iopub.status.idle": "2023-08-19T09:11:59.537781Z",
          "shell.execute_reply.started": "2023-08-19T09:11:58.497452Z",
          "shell.execute_reply": "2023-08-19T09:11:59.536316Z"
        },
        "trusted": true,
        "id": "AjGfxvMeaM6G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "[num_to_char[ch] for ch in [49, 40, 50, 46, 32, 32, 32]]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-08-06T07:01:13.448446Z",
          "iopub.execute_input": "2023-08-06T07:01:13.448834Z",
          "iopub.status.idle": "2023-08-06T07:01:13.457094Z",
          "shell.execute_reply.started": "2023-08-06T07:01:13.448805Z",
          "shell.execute_reply": "2023-08-06T07:01:13.455806Z"
        },
        "trusted": true,
        "id": "JVRsJqALaM6G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keras_model_converter = tf.lite.TFLiteConverter.from_keras_model(tflitemodel_base)\n",
        "keras_model_converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]\n",
        "\n",
        "tflite_model = keras_model_converter.convert()\n",
        "with open('model.tflite', 'wb') as f:\n",
        "    f.write(tflite_model)\n",
        "\n",
        "infargs = {\"selected_columns\": SEL_COLS}\n",
        "\n",
        "with open('inference_args.json', \"w\") as json_file:\n",
        "    json.dump(infargs, json_file)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-08-21T20:40:00.453210Z",
          "iopub.execute_input": "2023-08-21T20:40:00.453508Z",
          "iopub.status.idle": "2023-08-21T20:41:29.970608Z",
          "shell.execute_reply.started": "2023-08-21T20:40:00.453482Z",
          "shell.execute_reply": "2023-08-21T20:41:29.968924Z"
        },
        "trusted": true,
        "id": "F2c3KPexaM6G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "interpreter = tf.lite.Interpreter(\"model.tflite\")\n",
        "\n",
        "REQUIRED_SIGNATURE = \"serving_default\"\n",
        "REQUIRED_OUTPUT = \"outputs\"\n",
        "\n",
        "with open (\"/kaggle/input/asl-fingerspelling/character_to_prediction_index.json\", \"r\") as f:\n",
        "    character_map = json.load(f)\n",
        "rev_character_map = {j:i for i,j in character_map.items()}\n",
        "\n",
        "found_signatures = list(interpreter.get_signature_list().keys())\n",
        "\n",
        "if REQUIRED_SIGNATURE not in found_signatures:\n",
        "    raise KernelEvalException('Required input signature not found.')\n",
        "\n",
        "prediction_fn = interpreter.get_signature_runner(\"serving_default\")\n",
        "output = prediction_fn(inputs=tf.zeros((300, 273)))\n",
        "prediction_str = \"\".join([rev_character_map.get(s, \"\") for s in np.argmax(output[REQUIRED_OUTPUT], axis=1)])\n",
        "print(prediction_str)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-08-21T20:41:29.972895Z",
          "iopub.execute_input": "2023-08-21T20:41:29.973322Z",
          "iopub.status.idle": "2023-08-21T20:41:30.266653Z",
          "shell.execute_reply.started": "2023-08-21T20:41:29.973286Z",
          "shell.execute_reply": "2023-08-21T20:41:30.265298Z"
        },
        "trusted": true,
        "id": "urqDijm3aM6G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "def zip_files(file1, file2, output_zip):\n",
        "    with zipfile.ZipFile(output_zip, 'w') as zipf:\n",
        "        zipf.write(file1, 'inference_args.json')\n",
        "        zipf.write(file2, 'model.tflite')\n",
        "\n",
        "file1 = '/kaggle/working/inference_args.json'\n",
        "file2 = '/kaggle/working/model.tflite'\n",
        "output_zip = '/kaggle/working/submission.zip'\n",
        "\n",
        "zip_files(file1, file2, output_zip)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-08-21T20:41:30.267924Z",
          "iopub.execute_input": "2023-08-21T20:41:30.268267Z",
          "iopub.status.idle": "2023-08-21T20:41:30.398016Z",
          "shell.execute_reply.started": "2023-08-21T20:41:30.268238Z",
          "shell.execute_reply": "2023-08-21T20:41:30.396718Z"
        },
        "trusted": true,
        "id": "LZtDiJjIaM6H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate"
      ],
      "metadata": {
        "id": "GeGcPhbxaM6H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import json\n",
        "from tqdm.auto import tqdm\n",
        "import Levenshtein as Lev\n",
        "\n",
        "\n",
        "SEL_FEATURES = json.load(open('/kaggle/working/inference_args.json'))['selected_columns']\n",
        "\n",
        "def load_relevant_data_subset(pq_path):\n",
        "        return pd.read_parquet(pq_path, columns=SEL_FEATURES) #selected_columns)\n",
        "\n",
        "with open (\"/kaggle/input/asl-fingerspelling/character_to_prediction_index.json\", \"r\") as f:\n",
        "    character_map = json.load(f)\n",
        "rev_character_map = {j:i for i,j in character_map.items()}\n",
        "\n",
        "\n",
        "df = pd.read_csv('/kaggle/input/asl-fingerspelling/train.csv')\n",
        "\n",
        "idx = 0\n",
        "sample = df.loc[idx]\n",
        "loaded = load_relevant_data_subset('/kaggle/input/asl-fingerspelling/' + sample['path'])\n",
        "loaded = loaded[loaded.index==sample['sequence_id']].values\n",
        "print(loaded.shape)\n",
        "frames = loaded\n",
        "\n",
        "def wer__(s1, s2):\n",
        "    w1 = len(s1.split())\n",
        "    lvd = Lev.distance(s1, s2)\n",
        "    return lvd / w1\n",
        "\n",
        "found_signatures = list(interpreter.get_signature_list().keys())\n",
        "\n",
        "REQUIRED_SIGNATURE = 'serving_default'\n",
        "REQUIRED_OUTPUT = 'outputs'\n",
        "if REQUIRED_SIGNATURE not in found_signatures:\n",
        "    raise KernelEvalException('Required input signature not found.')\n",
        "\n",
        "prediction_fn = interpreter.get_signature_runner(\"serving_default\")\n",
        "output_lite = prediction_fn(inputs=frames)\n",
        "prediction_str = \"\".join([rev_character_map.get(s, \"\") for s in np.argmax(output_lite[REQUIRED_OUTPUT], axis=1)])\n",
        "print(prediction_str)\n",
        "\n",
        "\n",
        "st = time.time()\n",
        "cnt = 0\n",
        "total = 100\n",
        "model_time = 0\n",
        "\n",
        "levs = []\n",
        "\n",
        "for i in tqdm(range(len(df.iloc[:total]))):\n",
        "    sample = df.loc[i]\n",
        "    loaded = load_relevant_data_subset('/kaggle/input/asl-fingerspelling/' + sample['path'])\n",
        "    loaded = loaded[loaded.index==sample['sequence_id']].values\n",
        "\n",
        "    md_st = time.time()\n",
        "    output_ = prediction_fn(inputs=loaded)\n",
        "    model_time += time.time() - md_st\n",
        "\n",
        "\n",
        "    prediction_str = \"\".join([rev_character_map.get(s, \"\") for s in np.argmax(output_[REQUIRED_OUTPUT], axis=1)])\n",
        "    cur_lev = wer__(sample['phrase'], prediction_str)\n",
        "\n",
        "    levs.append(cur_lev)\n",
        "\n",
        "print(f'WER: {np.mean(levs):.5f}')\n",
        "print(f'Mean time: {(time.time() - st)/total:.7f}')\n",
        "print(f'Mean time only infer: {model_time/total:.7f}')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-08-16T09:51:37.770514Z",
          "iopub.execute_input": "2023-08-16T09:51:37.772472Z",
          "iopub.status.idle": "2023-08-16T09:52:54.992512Z",
          "shell.execute_reply.started": "2023-08-16T09:51:37.772404Z",
          "shell.execute_reply": "2023-08-16T09:52:54.990060Z"
        },
        "trusted": true,
        "id": "A1sM3PmmaM6H"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}